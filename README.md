# RAG Chatbot with LangGraph

A full-stack intelligent chatbot application that uses **Retrieval-Augmented Generation (RAG)** to answer questions based on your documents. Built with LangGraph, ChromaDB for vector storage, and Ollama for local LLM inference.

## What Does This App Do?

This chatbot application:
- ğŸ“š **Ingests and stores your documents** in a vector database (ChromaDB)
- ğŸ” **Retrieves relevant context** from documents when you ask questions
- ğŸ¤– **Generates intelligent answers** using a local LLM (via Ollama) combined with retrieved information
- ğŸ’¬ **Maintains conversation history** across multiple interactions
- ğŸŒ **Provides a web interface** for easy interaction

The RAG approach reduces hallucinations by grounding responses in your actual documents rather than relying solely on the LLM's training data.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Flask Frontend (app.py)                      â”‚ â”‚
â”‚  â”‚  - Web UI (templates/index.html)                         â”‚ â”‚
â”‚  â”‚  - Static assets (CSS/JS)                                â”‚ â”‚
â”‚  â”‚  - API proxy layer                                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ HTTP REST API
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BACKEND API LAYER                        â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              FastAPI Server (api.py)                      â”‚ â”‚
â”‚  â”‚  - /api/chat endpoint                                    â”‚ â”‚
â”‚  â”‚  - CORS middleware                                       â”‚ â”‚
â”‚  â”‚  - Request/Response validation                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ invoke
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LANGGRAPH WORKFLOW LAYER                    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                 LangGraph Workflow                        â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚     START â†’ retrieve â†’ generate â†’ END                    â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚  State: ChatState (messages, context)                    â”‚ â”‚
â”‚  â”‚  Memory: MemorySaver checkpointer                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                              â”‚
                 â”‚ retrieval                    â”‚ generation
                 â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     DATA LAYER               â”‚  â”‚      LLM LAYER              â”‚
â”‚                              â”‚  â”‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   ChromaDB             â”‚  â”‚  â”‚  â”‚   Ollama LLM          â”‚ â”‚
â”‚  â”‚  - Vector storage      â”‚  â”‚  â”‚  â”‚  - Local inference    â”‚ â”‚
â”‚  â”‚  - Hybrid search       â”‚  â”‚  â”‚  â”‚  - Model presets      â”‚ â”‚
â”‚  â”‚  - Document chunks     â”‚  â”‚  â”‚  â”‚  - Temperature config â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Setup Instructions

### Prerequisites

- **Python 3.11+**
- **Ollama** - For running local LLMs

### Step 1: Install Ollama

1. Visit [https://ollama.ai](https://ollama.ai)
2. Download and install for your OS
3. Start Ollama and pull a model:
   ```bash
   ollama serve
   ollama pull llama3.2
   ```

### Step 2: Set Up the Backend

1. Navigate to the backend directory:
   ```bash
   cd backend
   ```

2. Set up ChromaDB with your documents:
   ```bash
   python setup_chromadb.py
   ```
   
   This will:
   - Create the ChromaDB vector database
   - Ingest sample documents from `backend/data/`
   - Generate embeddings for semantic search

3. Run the start script (this handles everything else):
   ```bash
   start.bat
   ```
   
   This will:
   - Create a virtual environment
   - Install all required Python packages
   - Start the FastAPI backend server at `http://localhost:8000`

### Step 3: Set Up the Frontend

1. Open a new terminal and navigate to the frontend directory:
   ```bash
   cd frontend
   ```

2. Run the start script (this handles everything):
   ```bash
   start.bat
   ```
   
   This will:
   - Create a virtual environment
   - Install all required Python packages
   - Create a `.env` file from the template
   - Start the Flask frontend server at `http://localhost:5000`

### Step 4: Use the Application

1. Open your browser and go to `http://localhost:5000`
2. Type your question in the chat interface
3. The app will:
   - Search your documents for relevant information
   - Generate a context-aware response
   - Display the answer in the chat

## Adding Your Own Documents

1. Place your text files in `backend/data/`
2. Run the setup script again:
   ```bash
   cd backend
   python setup_chromadb.py
   ```
3. Your new documents will be indexed and available for querying

## Configuration

### Environment Variables

Create a `.env` file in the backend directory with the following variables:

```bash
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=qwen2.5-coder:0.5b

# LANGSMITH (Optional - for tracing and monitoring)
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_API_KEY=""
LANGSMITH_PROJECT="ips_hackaton_chatbot"

# AZURE (Optional - for Azure OpenAI integration)
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=

# API Configuration
API_BASE_URL=http://localhost:8000
```

### Model Options

You can use different Ollama models by changing the `DEFAULT_MODEL` setting:
- `llama3.2` - Fast, balanced performance
- `llama3.1` - Larger, more capable
- `mistral` - Alternative model option

Install additional models:
```bash
ollama pull mistral
```

## Project Structure

```
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api.py              # FastAPI server
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ setup_chromadb.py   # Database setup script
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ config/         # Model configuration
â”‚   â”‚   â””â”€â”€ graph/          # LangGraph workflow
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ chroma_db/      # Vector database storage
â”‚       â””â”€â”€ sample*.txt     # Sample documents
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py              # Flask application
â”‚   â”œâ”€â”€ templates/          # HTML templates
â”‚   â””â”€â”€ static/             # CSS and JavaScript
â””â”€â”€ README.md               # This file
```

## Technology Stack

- **Backend**: FastAPI, LangGraph, LangChain
- **Frontend**: Flask, Jinja2, JavaScript
- **Vector Database**: ChromaDB
- **LLM**: Ollama (local inference)
- **Workflow Engine**: LangGraph

## Troubleshooting

**Setup fails:**
- Run `start.bat` in both backend and frontend directories
- Make sure Python 3.11+ is installed and in your PATH
- Check your internet connection for downloading packages

**Dependency issues or need to reinstall libraries:**
- Run `backend\reinstall_libraries.bat` to clean and reinstall all backend dependencies
- This will remove the existing virtual environment and reinstall everything fresh

**Backend won't start:**
- Run `python setup_chromadb.py` first in the backend directory
- Then run `start.bat` in the backend directory
- Ensure Ollama is running and has a model installed

**Ollama connection errors:**
- Verify Ollama is running: `ollama serve`
- Check the model is installed: `ollama list`
- Confirm OLLAMA_BASE_URL in your `.env` file

**ChromaDB errors:**
- Delete `backend/data/chroma_db/` and run `python setup_chromadb.py` again
- Check file permissions in the data directory

**Frontend can't connect to backend:**
- Ensure backend is running on port 8000
- Verify API_BASE_URL in frontend configuration
- Check CORS settings in `backend/api.py`

## Features

âœ… RAG-based question answering  
âœ… Vector similarity search  
âœ… Conversational memory  
âœ… Local LLM inference (privacy-friendly)  
âœ… Web-based user interface  
âœ… Easy document ingestion  
âœ… Customizable model settings  

## License

MIT License - feel free to use and modify for your needs.
